---
title: "Week 5: Regression Via Matrix Operations"
author: "Stat 431"
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source(here::here("Canvas_Pages/Scripts/window_dressing.R"))
quiz_url <- "https://canvas.calpoly.edu/courses/16654/quizzes/14543"
library(tidyverse)
library(flair)
```

```{css, echo = FALSE}
img {
  vertical-align:middle
}

body {
  text-shadow:none;
}


```


```{r, results = "asis", echo = FALSE}
make_summary_table(vid_time = "13 min", 
                   reading_time = "30 min", 
                   work_time = "60 min", 
                   canvas_activities = 2)
```

## Simple Linear Regression

In your prerequisite statistics coursework you undoubtedly spent some time on the topic of regression. The idea is that we have a response variable and at least one explanatory variable that is suspected to be related to the response in some way we'd like to model.

In **simple linear regression** there is just one explanatory variable and the model we're estimating has an assumed straight-line form. In general, **multiple regression** extends this idea to accommodate many different complex relationships between a response and any number of explanatory variables, all within a single model.

It's likely that you used software like R, JMP, or Minitab to estimate the regression model after specifying which variable was your response and which variables were your explanatory variables (predictors). There are many nice functions for performing regression, but today you'll learn about how to do it yourself!

We're not going to go through the mathematical derivations, but now that you've gained some knowledge in matrix operations in R it should be relatively straightforward to implement regression in R. The video, however, does go into the mathematics of estimating the regression coefficients. **You are not responsible for knowing the calculus and linear algebra used in the video.**

```{r, results = "asis", echo = FALSE}
req_vid("Linear Regression with Matrices", url = 'https://www.youtube.com/embed/Qa_FI92_qo8')
```

Notice that this video didn't actually involve any R! However, it makes the computations required to estimate regression coefficients extremely clear.

```{r, results = "asis", echo = FALSE}
checkin("Regression with Matrices")
```

a) What is the column of 1's in the X matrix needed for?

* To compute the residuals
* To include an intercept in the model
* So that we only include each observation once
* Toothbrush

b) The rows of the X matrix represent _____.

* explanatory variables
* the response variable
* observations
* residuals

c) Including more than one explanatory variable in the model means the X matrix _____.

* will have more rows
* will have more columns
* will have more rows and more columns

d) After we've computed the coefficient estimates, what does XA represent?

* the residuals
* the response values
* the fitted values

e) In the slide shown at 3:18, there is a typo in the regression equation.

* True
* False


```{r, results = "asis", echo = FALSE}
canvas_task(quiz_url)
```

## Ridge Regression

There are all sorts of things that can go wrong when performing regression! Unfortunately, it's a bit beyond the scope of this class to go into them all. So, we'll encourage you to explore more on your own or take more statistics courses like Cal Poly's STAT 419 or STAT 434.

One of the things that can be challenging when performing traditional linear regression is having a **very large number of explanatory variables.** It's even possible to have more explanatory variables than observations! However, if this is then case then parts of the matrix algebra needed to estimate the coefficients will not work. One method for dealing with this challenge is known as **regularization** or **penalized regression**. There are a couple of specific, but popular models that fall under this umbrella: **ridge regression** and **the lasso**. 

You're going to learn a bit more about ridge regression, including how to implement it! The basic idea is that we want to fit the regression model in such a way as to end up not including every single explanatory variable we have in our dataset. To do this, **ridge regression** penalizes coefficients for being large in magnitude (i.e. very non-zero) but still tries to minimize the error (sum of squared residuals) just like traditional regression did.

This may sound a bit complicated, but it actually simplifies somewhat nicely. Check out the reading below for more details!

```{r, results = "asis", echo = FALSE}
req_read("Ridge Regression Introduction", url = "https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db")
```

**Notes:**

- One of the primary gems of this reading is the expression in the section titled "What We Really Want to Find". This expression should look very familiar, though slightly different.

```{r, results = "asis", echo = FALSE}
checkin("Ridge Regression")
```

f) As we increase the value of $\lambda$, what happens to the coefficient estimates?

* They increase in magnitude
* They decrease in magnitude
* They stay the same

g) In prediction, how does ridge regression generally perform relative to traditional linear regression?

* Better
* Worse
* The same

h) Will any of the regression coefficients ever actually reach zero?

* Yes
* No
* Maybe?


```{r, results = "asis", echo = FALSE}
canvas_task(quiz_url)
```

```{r, results = "asis", echo = FALSE}
extra_rec("")
```

* [More Details on Ridge Regression](https://en.wikipedia.org/wiki/Tikhonov_regularization)



